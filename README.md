[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/Lj3YlzJp)
# Proyecto Final 2025-1: Desarrollo de un Clasificador Neuronal Multiclase desde C++
## **CS1103 Programaci√≥n III** ¬∑ Informe Final

VIDEO DE PRESENTACION: https://drive.google.com/file/d/1Jv4BrKS0BFECl6xLnw_O-fsP1_dKcS__/view?usp=sharing
### **Descripci√≥n**

Reconocimiento de D√≠gitos Manuscritos mediante Redes Neuronales Multicapa

### Contenidos

1. [Datos generales](#datos-generales)  
2. [Requisitos e instalaci√≥n](#requisitos-e-instalaci√≥n)  
3. [Investigaci√≥n te√≥rica](#1-investigaci√≥n-te√≥rica)  
4. [Dise√±o e implementaci√≥n](#2-dise√±o-e-implementaci√≥n)  
5. [Ejecuci√≥n](#3-ejecuci√≥n)  
6. [An√°lisis del rendimiento](#4-an√°lisis-del-rendimiento)  
7. [Conclusiones](#5-conclusiones)  
8. [Bibliograf√≠a](#6-bibliograf√≠a)  
---

### Datos generales

* **Tema**: Clasificaci√≥n de d√≠gitos manuscritos con redes neuronales en C++
* **Grupo**: `grupogpt`
* **Integrantes**:

  * Rodrigo Miguel Gomez Pacheco - 202410309
  * Javier Alejandro Castro Barreto - 202310081
  * Julio Ruiz Villavicencio - 202410616
---

### Requisitos e instalaci√≥n

Para compilar y ejecutar correctamente el proyecto, es necesario contar con los siguientes requisitos:

#### 1. Compilador

* GCC 11 o superior (compatible con C++17)

#### 2. Dependencias

* [CMake ‚â• 3.18](https://cmake.org/)
* [Eigen 3.4](https://eigen.tuxfamily.org/) (incluida localmente en el repositorio)
* STL est√°ndar de C++

#### 3. Clonaci√≥n del repositorio

bash
git clone https://github.com/CS1103/projecto-final-grupogpt.git
cd projecto-final-grupogpt

---

### 1. Investigaci√≥n te√≥rica

### 1.1.1. Fundamentos de Espacios Vectoriales y Transformaciones Lineales

Los espacios vectoriales son estructuras matem√°ticas que permiten representar datos y operaciones de forma abstracta, y resultan esenciales en disciplinas como la computaci√≥n, f√≠sica e ingenier√≠a. Las transformaciones lineales, por su parte, modelan c√≥mo cambian estos vectores bajo distintas operaciones, preservando propiedades como la aditividad y homogeneidad. Seg√∫n Lay, Lay y McDonald en *Linear Algebra and Its Applications* (2015), la comprensi√≥n de estos fundamentos permite construir modelos matem√°ticos robustos para la manipulaci√≥n de datos en m√∫ltiples dimensiones, siendo base para algoritmos modernos de reconocimiento de patrones y compresi√≥n de informaci√≥n.

### 1.1.2. C√°lculo de Derivadas Parciales y Gradientes en Optimizaci√≥n

El c√°lculo multivariable, y en particular las derivadas parciales, permiten analizar c√≥mo una funci√≥n cambia respecto a cada una de sus variables independientes. Cuando estas derivadas se agrupan en un vector ‚Äîel gradiente‚Äî, se obtiene una herramienta clave para minimizar funciones objetivo mediante m√©todos iterativos. Boyd y Vandenberghe, en *Convex Optimization* (2004), resaltan el rol del gradiente como direcci√≥n de descenso m√°s pronunciada, lo cual es esencial para algoritmos como el descenso de gradiente, utilizados ampliamente en estad√≠stica, inteligencia artificial y simulaciones num√©ricas.

### 1.1.3. Computaci√≥n Num√©rica en Ambientes de Bajo Nivel

En el desarrollo de algoritmos de aprendizaje autom√°tico desde cero, la manipulaci√≥n eficiente de estructuras de datos como matrices es esencial. A diferencia de entornos de alto nivel como Python con NumPy o TensorFlow, en C++ la gesti√≥n de memoria, operaciones aritm√©ticas y control de flujo deben ser implementadas de forma expl√≠cita. Esto obliga a un conocimiento m√°s profundo del √°lgebra lineal computacional. Seg√∫n Golub y Van Loan (2013), las operaciones b√°sicas sobre matrices como productos punto, transposici√≥n y normalizaci√≥n son la base sobre la cual se construyen modelos de redes neuronales, y su correcta implementaci√≥n determina el rendimiento num√©rico y la estabilidad del entrenamiento.

### 1.1.4. Representaci√≥n de Datos y Tipos Gen√©ricos en Modelos Neuronales

Una red neuronal requiere representar grandes vol√∫menes de datos num√©ricos de forma estructurada. El uso de plantillas (`templates`) en C++ permite dise√±ar estructuras gen√©ricas como tensores o capas densas que funcionen indistintamente sobre tipos de precisi√≥n simple (`float`) o doble (`double`). Esta capacidad de abstracci√≥n es vital cuando se implementan redes neuronales que deben balancear precisi√≥n con rendimiento, como afirman Meyers (2014) y Alexandrescu (2001). En este contexto, cada componente del modelo como los pesos de una capa, los gradientes o las salidas intermedias puede tratarse como una matriz multidimensional, operando bajo un marco matem√°tico riguroso pero optimizado para eficiencia computacional.

### 1.1.5. Dataset MNIST: Referente Cl√°sico para la Clasificaci√≥n de D√≠gitos

El dataset MNIST (Modified National Institute of Standards and Technology) es una colecci√≥n ampliamente utilizada en el √°mbito del aprendizaje autom√°tico para tareas de clasificaci√≥n de im√°genes. Introducido por LeCun et al. (1998), MNIST contiene un conjunto de 70,000 im√°genes en escala de grises de d√≠gitos manuscritos del 0 al 9, donde cada imagen posee una resoluci√≥n de 28x28 p√≠xeles. De estas, 60,000 se destinan al entrenamiento y 10,000 a la evaluaci√≥n del modelo.

Cada imagen est√° acompa√±ada por una etiqueta que indica el d√≠gito correspondiente, lo cual lo convierte en un problema de clasificaci√≥n multiclase supervisada con 10 clases posibles. Su popularidad se debe a la simplicidad del formato, la limpieza de los datos y su idoneidad para validar arquitecturas b√°sicas de redes neuronales como perceptrones multicapa (MLP) y redes convolucionales (CNN).

### 6. Implementaci√≥n del Clasificador Neuronal desde Cero

El presente proyecto implementa un sistema de reconocimiento de d√≠gitos manuscritos mediante una red neuronal artificial construida √≠ntegramente en C++. La arquitectura general corresponde a un perceptr√≥n multicapa (MLP), donde se emplean capas densas (`DenseLayer`) y funciones de activaci√≥n no lineales (`ReLU` y `Softmax`) para el procesamiento y clasificaci√≥n de entradas.

La implementaci√≥n se ha dividido modularmente en distintos archivos fuente que encapsulan funcionalidades espec√≠ficas. La clase `NeuralNetwork` gestiona la secuencia de capas, el proceso de entrenamiento, y la retropropagaci√≥n del error, apoy√°ndose en interfaces (`ILayer`, `IActivation`, `ILoss`) que promueven un dise√±o extensible. Asimismo, la clase `Tensor` permite representar vectores y matrices de forma eficiente, facilitando las operaciones algebraicas fundamentales para el forward y backward pass.

Los datos de entrada provienen del dataset MNIST en formato `.csv`, los cuales son procesados mediante la clase `MNISTLoader`, responsable de cargar, normalizar y preparar los tensores de entrada y las etiquetas one-hot para el entrenamiento. El entrenamiento se realiza con descenso de gradiente estoc√°stico (`SGD`), implementado en el optimizador `SGDOptimizer`, que actualiza los pesos tras cada √©poca.

Durante la ejecuci√≥n, el modelo se entrena con `mnist_train.csv` y se valida con `mnist_test.csv`, mostrando m√©tricas como la precisi√≥n (accuracy) por √©poca y la p√©rdida (loss) acumulada. La estructura modular, combinada con pruebas automatizadas, permite escalar o adaptar f√°cilmente el modelo a datasets similares o arquitecturas m√°s complejas.

Este proyecto demuestra la viabilidad de desarrollar desde cero una red neuronal funcional en C++, sin el uso de frameworks externos como TensorFlow o PyTorch, contribuyendo as√≠ al entendimiento profundo de los mecanismos internos del aprendizaje supervisado.

---

### 2. Dise√±o e implementaci√≥n

#### 2.1 Arquitectura de la soluci√≥n

El proyecto se ha estructurado siguiendo una arquitectura modular, lo cual facilita la escalabilidad, el mantenimiento del c√≥digo y la incorporaci√≥n de futuras mejoras. Se ha aplicado el principio de separaci√≥n de responsabilidades, distribuyendo las funcionalidades en archivos fuente bien delimitados, cada uno con una responsabilidad espec√≠fica.

* **Principales m√≥dulos del sistema**:

 ### üìÑ `neural_network.h`

**Descripci√≥n:**  
Este archivo define la clase `NeuralNetwork`, que representa la estructura principal de la red neuronal multicapa (MLP).

**Responsabilidad principal:**  
Gestionar la **secuencia de capas** de la red y coordinar el flujo de datos durante las fases de **propagaci√≥n hacia adelante (forward)** y **retropropagaci√≥n del error (backward)**.

**Caracter√≠sticas clave:**

- Utiliza un vector de punteros inteligentes (`std::unique_ptr`) a la interfaz `ILayer<T>`, lo que permite almacenar distintos tipos de capas (densas, activaci√≥n, etc.) de forma polim√≥rfica.
- El m√©todo `add_layer()` permite construir la red a√±adiendo capas din√°micamente.
- El m√©todo `forward()` propaga una entrada a trav√©s de todas las capas y devuelve la salida.
- El m√©todo `backward()` recorre las capas en orden inverso, propagando el gradiente hacia atr√°s.
- `predict()` es simplemente un alias de `forward()`, pensado para la fase de inferencia.
- La red es **modular** y **extensible**, ya que depende solo de la interfaz `ILayer`.

**Relaci√≥n con otros archivos:**

- Incluye cabeceras como `nn_dense.h`, `nn_activation.h`, `nn_loss.h` y `nn_optimizer.h`, lo que indica que cada capa espec√≠fica (como `DenseLayer`, `ReLU`, etc.) implementa la interfaz com√∫n `ILayer<T>`.
- Utiliza la clase `Tensor2D<T>` como tipo de entrada/salida para representar los datos (probablemente un alias de `Tensor<T, 2>`).

### üìÑ `nn_dense.h`

**Descripci√≥n:**  
Este archivo implementa la clase `Dense`, que representa una **capa densa (fully connected)** en la red neuronal. Es una de las capas principales del modelo MLP.

**Responsabilidad principal:**  
Realizar el producto matricial entre la entrada y los pesos (`_weights`), sumar los sesgos (`_biases`) y propagar el resultado hacia la siguiente capa. Tambi√©n se encarga de calcular los gradientes y actualizar los par√°metros durante el entrenamiento.

**Caracter√≠sticas clave:**

- Inicializa los pesos con una distribuci√≥n normal escalada por la dimensi√≥n de entrada (He initialization).
- Implementa los m√©todos:
  - `forward()`: Propagaci√≥n hacia adelante.
  - `backward()`: Retropropagaci√≥n del error, calculando gradientes de pesos y sesgos.
  - `update_params()`: Actualiza los par√°metros usando un optimizador (como SGD).
- Soporta carga y guardado de pesos (`save_weights` y `load_weights`) para persistencia del modelo.
- Guarda el √∫ltimo input (`_last_input`) para usarlo en la retropropagaci√≥n.

**Relaci√≥n con otros archivos:**

- Depende de `nn_interfaces.h`, ya que hereda de la interfaz `ILayer<T>`.
- Utiliza `Tensor2D<T>` para representar entradas, pesos y gradientes.
- Colabora con `IOptimizer` (ej. `SGDOptimizer`) para actualizar sus par√°metros.

### üìÑ `nn_activation.h`

**Descripci√≥n:**  
Este archivo define la clase `ReLU`, una funci√≥n de activaci√≥n usada com√∫nmente en redes neuronales profundas. Implementa la interfaz `ILayer<T>`, lo que le permite integrarse como una capa m√°s dentro de la red.

**Responsabilidad principal:**  
Aplicar la funci√≥n de activaci√≥n **ReLU (Rectified Linear Unit)** durante la propagaci√≥n hacia adelante, y su derivada durante la retropropagaci√≥n.

**Caracter√≠sticas clave:**

- En el m√©todo `forward()`, reemplaza los valores negativos por cero, conservando los positivos.
- Utiliza una **m√°scara (`_mask`)** para almacenar qu√© entradas fueron mayores que cero, lo cual se usa en la fase `backward()` para derivar correctamente.
- En `backward()`, propaga el gradiente solo donde la entrada original fue positiva (seg√∫n la m√°scara).

**Relaci√≥n con otros archivos:**

- Hereda de la interfaz `ILayer<T>`, definida en `nn_interfaces.h`, lo que permite su uso dentro de la red definida en `neural_network.h`.
- Utiliza `Tensor2D<T>` como estructura para manejar las matrices de activaci√≥n y gradientes.

### üìÑ `nn_loss.h`

**Descripci√≥n:**  
Este archivo implementa la clase `SoftmaxCrossEntropyLoss`, que combina la funci√≥n de activaci√≥n **Softmax** con la funci√≥n de p√©rdida **Entrop√≠a Cruzada (Cross-Entropy)**. Es ideal para tareas de **clasificaci√≥n multiclase**, como el reconocimiento de d√≠gitos en MNIST.

**Responsabilidad principal:**  
- Calcular la **p√©rdida** entre las predicciones (`logits`) y las etiquetas reales codificadas en **one-hot**.
- Calcular el **gradiente** necesario para retropropagaci√≥n en el entrenamiento.

**Caracter√≠sticas clave:**

- Realiza el c√°lculo de **Softmax** de forma num√©ricamente estable (usando el truco de restar el valor m√°ximo por fila).
- Evita problemas de precisi√≥n al calcular logaritmos mediante una constante `epsilon` (`std::numeric_limits<T>::epsilon()`).
- Guarda internamente:
  - `_softmax_outputs`: resultados de la activaci√≥n softmax.
  - `_last_targets`: etiquetas reales del batch.
- El m√©todo `forward()` devuelve la p√©rdida promedio por batch.
- El m√©todo `backward()` devuelve el gradiente de la p√©rdida respecto a las salidas (`softmax_outputs - targets`), ya que esta combinaci√≥n permite una derivada simplificada y eficiente.

**Relaci√≥n con otros archivos:**

- Utiliza la clase `Tensor2D<T>` como estructura principal para representar matrices de entrada, salida y gradiente.
- Se integra con la red definida en `neural_network.h` y se usa t√≠picamente despu√©s de la √∫ltima capa (por ejemplo, despu√©s de `Dense` y `Softmax` impl√≠cito).

### üìÑ `nn_optimizer.h`

**Descripci√≥n:**  
Este archivo define dos algoritmos de optimizaci√≥n: **SGD (Stochastic Gradient Descent)** y **Adam**, ambos implementando la interfaz `IOptimizer<T>`. Estos optimizadores actualizan los pesos y sesgos de las capas entrenables usando los gradientes calculados durante la retropropagaci√≥n.

**Responsabilidad principal:**  
Aplicar reglas de actualizaci√≥n a los par√°metros del modelo (pesos y sesgos) con base en los gradientes y una tasa de aprendizaje.

---

#### üîπ `SGD` (Stochastic Gradient Descent)

- Algoritmo de optimizaci√≥n m√°s simple.
- La clase `SGD` recibe una tasa de aprendizaje (`_lr`) y actualiza los par√°metros seg√∫n la f√≥rmula:  
  \[
  \text{param} = \text{param} - \text{lr} \times \text{grad}
  \]
- M√©todo: `update(Tensor<T,2>& params, const Tensor<T,2>& grads)`

---

#### üîπ `Adam` (Adaptive Moment Estimation)

- Optimizador avanzado que adapta la tasa de aprendizaje para cada par√°metro.
- Utiliza momentos de primer orden (**m**, promedio de los gradientes) y segundo orden (**v**, promedio de los gradientes al cuadrado).
- Aplica correcci√≥n de sesgo (`m_hat`, `v_hat`) en cada iteraci√≥n `t`.
- Internamente maneja vectores `m`, `v` y contador `t` usando `thread_local`, lo que permite mantener el estado del optimizador por hilo.

**F√≥rmula de actualizaci√≥n usada:**
\[
\theta = \theta - \eta \cdot \frac{m_t}{\sqrt{v_t} + \varepsilon}
\]

**Relaci√≥n con otros archivos:**

- Se usa desde las capas entrenables como `Dense` mediante el m√©todo `update_params()` del `ILayer<T>`.
- Compatible con cualquier estructura de par√°metros basada en `Tensor2D<T>`.

### üìÑ `mnist_loader.h`

**Descripci√≥n:**  
Este archivo implementa una utilidad para cargar datos del conjunto **MNIST** desde archivos `.csv`, formate√°ndolos como tensores num√©ricos compatibles con la red neuronal.

**Responsabilidad principal:**  
Leer y procesar los datos de im√°genes y etiquetas desde el archivo CSV, normalizarlos y representarlos como tensores `Tensor2D<double>` que se usar√°n como entradas (`images`) y salidas (`labels`) en el entrenamiento del modelo.

**Caracter√≠sticas clave:**

- La funci√≥n `load_mnist_csv()`:
  - Recibe la ruta del archivo `.csv` y el n√∫mero de im√°genes a cargar.
  - Convierte las im√°genes en escala de grises de 28x28 (784 p√≠xeles) a un tensor normalizado entre 0 y 1.
  - Convierte las etiquetas (d√≠gitos del 0 al 9) a codificaci√≥n **one-hot** de 10 dimensiones.
- Usa la funci√≥n auxiliar `split()` para dividir cada l√≠nea del CSV.
- Devuelve un `std::pair<Tensor2D_d, Tensor2D_d>` que representa:  
  `‚ü∂ (im√°genes_normalizadas, etiquetas_one_hot)`

**Relaci√≥n con otros archivos:**

- Utiliza la clase `Tensor2D` (definida en `tensor.h`) para almacenar los datos cargados.
- Es utilizada al inicio del entrenamiento para preparar los datos provenientes de `mnist_train.csv` y `mnist_test.csv`.

**Ejemplo de uso en entrenamiento:**

```cpp
auto [train_images, train_labels] = utec::data::load_mnist_csv("mnist_train.csv", 60000);
auto [test_images, test_labels] = utec::data::load_mnist_csv("mnist_test.csv", 10000);
```

### üìÑ `tensor.h`

**Descripci√≥n:**  
Este archivo implementa una estructura de datos gen√©rica llamada `Tensor`, utilizada para representar arreglos multidimensionales (vectores, matrices, etc.) de cualquier tipo num√©rico. Constituye la base del c√°lculo algebraico dentro del modelo neuronal.

**Responsabilidad principal:**  
Proporcionar un contenedor flexible y eficiente para almacenar datos y realizar operaciones matem√°ticas esenciales como suma, resta, multiplicaci√≥n escalar, broadcasting, transposici√≥n y multiplicaci√≥n de matrices. Es indispensable para la propagaci√≥n hacia adelante y hacia atr√°s en la red neuronal.

**Caracter√≠sticas clave:**

- Template general `Tensor<T, Rank>` para manejar tensores de cualquier tipo y dimensi√≥n (`Rank`).
- M√©todos sobrecargados para:
  - Acceso multidimensional con `operator()`.
  - Acceso lineal con `operator[]`.
  - Operaciones aritm√©ticas `+`, `-`, `*`, `/`.
  - Transposici√≥n (`transpose_2d()`) y multiplicaci√≥n matricial (`matmul()`).
- Internamente usa `std::vector<T>` como almacenamiento lineal y `std::array<size_t, Rank>` para la forma (`shape`) y los `strides`.
- Funci√≥n de impresi√≥n `operator<<` para visualizaci√≥n directa de tensores por consola.

**Utilidad en el proyecto:**

- Es el tipo base sobre el cual operan las capas (`Dense`, `ReLU`, etc.) y el optimizador (`SGDOptimizer`).
- Permite calcular gradientes, productos matriciales y mantener consistencia dimensional durante el entrenamiento.
- Aporta abstracci√≥n matem√°tica sin depender de librer√≠as externas como Eigen o Armadillo.

**Ejemplo de uso:**

```cpp
Tensor<double, 2> A(3, 4);     // Tensor de 2D con 3 filas y 4 columnas
A.fill(1.0);                   // Llenar con unos
auto B = A.transpose_2d();     // Transponer A
```

### üìÑ `common_helpers.h`

**Descripci√≥n:**  
Este archivo contiene funciones auxiliares utilizadas para evaluar el rendimiento del modelo y extraer predicciones. Proporciona herramientas pr√°cticas para el flujo de pruebas y validaci√≥n, especialmente despu√©s del entrenamiento.

**Responsabilidad principal:**  
- Determinar la clase predicha a partir del vector de salida de la red.
- Evaluar el modelo completo sobre el conjunto de prueba, mostrando el **accuracy** total.

**Funciones principales:**

- `get_predicted_class(prediction)`:  
  Recibe un tensor de salida (por ejemplo, de tama√±o `1x10`) y devuelve el √≠ndice con mayor probabilidad, usando un **argmax**.
  
- `evaluate(model, test_images, test_labels)`:  
  - Recorre todas las im√°genes de prueba y predice la clase utilizando `model.predict()`.
  - Compara con la etiqueta real (tambi√©n extra√≠da con argmax).
  - Muestra la **exactitud (accuracy)** como porcentaje en consola.

**Relaci√≥n con otros archivos:**

- Usa la clase `NeuralNetwork<double>` definida en `neural_network.h`.
- Utiliza la clase `Tensor2D<double>` definida en `tensor.h`.
- Ideal para su uso en el `main()` al final del entrenamiento para validar el desempe√±o del modelo.

**Ejemplo de uso:**

```cpp
evaluate(model, test_images, test_labels);
// Output: Accuracy: 91.75%
```

### üìÑ `image_processor.h`

**Descripci√≥n:**  
Este m√≥dulo proporciona funcionalidades para procesar im√°genes externas (como PNG) y convertirlas en vectores de entrada adecuados para la red neuronal. Est√° dise√±ado especialmente para experimentar con im√°genes reales de d√≠gitos manuscritos, fuera del dataset MNIST.

**Responsabilidad principal:**  
Leer una imagen desde disco, convertirla a escala de grises, binarizarla, encontrar su contorno, redimensionarla a 28x28 p√≠xeles, centrarla y normalizar sus valores para que pueda ser utilizada como entrada para la red neuronal.

**Funciones principales:**

- `preprocess_image_stb(filepath)`  
  - Carga la imagen usando `stb_image`.
  - Convierte a escala de grises.
  - Binariza con umbral (`thr = 60`).
  - Aplica una dilataci√≥n para agrandar el trazo del d√≠gito.
  - Calcula una bounding box del d√≠gito.
  - Recorta la regi√≥n relevante, la redimensiona a 20√ó20 y la centra en una imagen de 28√ó28.
  - Normaliza los valores entre 0 y 1 y devuelve un `Tensor2D<double>` listo para predecir.

- `print_ascii_28x28(tensor)`  
  - Imprime una representaci√≥n ASCII del tensor 28√ó28.
  - √ötil para verificar visualmente si el d√≠gito fue correctamente procesado.

**Relaci√≥n con otros m√≥dulos:**

- Usa `Tensor2D<double>` definido en `tensor.h`.
- Usa las bibliotecas externas `stb_image.h` y `stb_image_resize.h`.

**Ejemplo de uso:**

```cpp
auto input_tensor = utec::utils::preprocess_image_stb("mi_digito.png");
print_ascii_28x28(input_tensor);
auto prediction = model.predict(input_tensor);
```

**Dependencias externas:**
- `stb_image.h` y `stb_image_resize.h` (de `https://github.com/nothings/stb`) para la lectura y redimensionamiento de im√°genes PNG.

- `stb_image.h` y `stb_image_resize.h`: Librer√≠as externas utilizadas para el procesamiento de im√°genes en formato PNG.

**Dise√±o orientado a interfaces**:

El sistema incorpora interfaces gen√©ricas como `ILayer`, `IActivation`, `ILoss` y `IOptimizer`, que permiten desacoplar las implementaciones concretas y seguir principios de dise√±o como el Open/Closed (abierto a extensi√≥n, cerrado a modificaci√≥n). Este enfoque posibilita extender el sistema con nuevas capas, funciones o m√©todos de entrenamiento sin alterar la estructura central.

## üìÅ Estructura del Proyecto

```plaintext
projecto-final-grupogpt/
‚îú‚îÄ‚îÄ .gitignore                    # Archivos/directorios ignorados por Git
‚îú‚îÄ‚îÄ CMakeLists.txt                # Configuraci√≥n de compilaci√≥n con CMake
‚îú‚îÄ‚îÄ README.md                     # Documentaci√≥n del proyecto
‚îú‚îÄ‚îÄ mnist_loader.h               # Carga y preprocesamiento del dataset MNIST
‚îú‚îÄ‚îÄ predict.cpp                  # Ejecuta predicciones sobre nuevas im√°genes
‚îú‚îÄ‚îÄ train.cpp                    # Entrenamiento de la red neuronal
‚îú‚îÄ‚îÄ stb_image.h                  # Librer√≠a externa para cargar im√°genes PNG
‚îú‚îÄ‚îÄ stb_image_resize.h           # Librer√≠a externa para redimensionar im√°genes PNG

‚îú‚îÄ‚îÄ Imagenes_Prueba/             # Im√°genes PNG para pruebas de predicci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep

‚îú‚îÄ‚îÄ data/                        # Datos y recursos auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ data.zip
‚îÇ   ‚îî‚îÄ‚îÄ model_architecture.txt   # Estructura textual de la red neuronal entrenada

‚îú‚îÄ‚îÄ layer_output/                # Pesos y sesgos guardados tras entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ layer0_weights.txt
‚îÇ   ‚îú‚îÄ‚îÄ layer0_biases.txt
‚îÇ   ‚îú‚îÄ‚îÄ layer2_weights.txt
‚îÇ   ‚îú‚îÄ‚îÄ layer2_biases.txt
‚îÇ   ‚îú‚îÄ‚îÄ layer4_weights.txt
‚îÇ   ‚îî‚îÄ‚îÄ layer4_biases.txt

‚îú‚îÄ‚îÄ nn/                          # N√∫cleo del sistema de red neuronal
‚îÇ   ‚îú‚îÄ‚îÄ neural_network.h         # Clase central que orquesta las capas y el entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ nn_activation.h          # Funciones de activaci√≥n (ReLU, Softmax)
‚îÇ   ‚îú‚îÄ‚îÄ nn_dense.h               # Capas densas totalmente conectadas
‚îÇ   ‚îú‚îÄ‚îÄ nn_interfaces.h          # Interfaces base para capas, funciones de p√©rdida, etc.
‚îÇ   ‚îú‚îÄ‚îÄ nn_loss.h                # Funci√≥n de p√©rdida: CrossEntropy
‚îÇ   ‚îî‚îÄ‚îÄ nn_optimizer.h           # Optimizador: Descenso de Gradiente Estoc√°stico (SGD)

‚îú‚îÄ‚îÄ utils/                       # M√≥dulos auxiliares y utilitarios
‚îÇ   ‚îú‚îÄ‚îÄ ascii_view.h             # Visualizaci√≥n de im√°genes en formato ASCII
‚îÇ   ‚îú‚îÄ‚îÄ common_helpers.h         # Funciones auxiliares para m√©tricas y evaluaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ image_processor.h        # Conversi√≥n de im√°genes PNG a tensores
‚îÇ   ‚îî‚îÄ‚îÄ tensor.h                 # Implementaci√≥n gen√©rica de tensores (N-dimensionales)
```

## 3. Ejecuci√≥n

**Estado:**  
El sistema compila y se ejecuta correctamente. Se entren√≥ y evalu√≥ sobre el dataset MNIST, mostrando predicciones y errores visuales en consola.

```
bash
cd build
cmake ..
make
./train

```

Se prepararon correctamente los archivos mnist_train.csv y mnist_test.csv en formato CSV.

Se ejecut√≥ el programa train, el cual:

    Carga los datos de manera secuencial.

    Inicializa una red neuronal con la siguiente arquitectura:
    784 ‚Üí 128 ‚Üí 64 ‚Üí 10.

    Utiliza codificaci√≥n one-hot para las etiquetas (Y_train, Y_test).

    Entrena la red durante 2 √©pocas con batch_size = 100 y learning_rate = 0.001.

    Imprime 2 aciertos y 1 error visualizado en consola mediante caracteres ASCII.

    Guarda la arquitectura y los pesos entrenados en archivos .txt.



### 3.2 Inferencia (`predict`)

**Pasos de ejecuci√≥n:**

```bash
cd build
./predict
```
Flujo de uso real:

    El programa predict no requiere volver a entrenar la red.

    Carga autom√°ticamente los pesos y arquitectura previamente guardados por el programa train.

Fuentes de entrada para predicci√≥n:

    Im√°genes del archivo mnist_test.csv (muestras est√°ndar de validaci√≥n).

    Dibujos propios del usuario almacenados como im√°genes dentro de la carpeta Imagenes_Prueba/.

Requisitos para im√°genes propias:

    Las im√°genes pueden estar en distintos tama√±os iniciales, pero:

        El sistema redimensiona autom√°ticamente a 28x28 p√≠xeles.

        El d√≠gito se centra autom√°ticamente en la imagen.

        Se recomienda que el n√∫mero tenga un grosor suficiente para evitar errores por irregularidades de trazo.

    El preprocesamiento incluye:

        Escalado a escala de grises.

        Normalizaci√≥n de valores.

        Conversi√≥n al formato tensorial compatible con la red neuronal.

Salida esperada:

    Visualizaci√≥n en consola de:

        El n√∫mero predicho por la red.

        La imagen original representada en ASCII, √∫til para validar visualmente aciertos y errores.


---

## 4. An√°lisis del rendimiento

### 4.1 M√©tricas reales observadas

- üß≠ **√âpocas ejecutadas:** 2  
- ‚è±Ô∏è **Tiempo estimado por √©poca:** 600 segundos  
- üéØ **Precisi√≥n obtenida en el test set:** > 90 %  
- üìâ **Funci√≥n de p√©rdida:** decreciente por √©poca  

---

### 4.2 Observaciones adicionales

- El sistema mostr√≥ **buen rendimiento con las im√°genes del dataset original MNIST**.
- Sin embargo, present√≥ **dificultades al predecir correctamente d√≠gitos como 9 y 7** cuando se utilizaron dibujos propios (inputs externos al dataset).
- Esto sugiere que la red tiene una **sensibilidad particular a ciertas formas o estilos no representados** en los datos de entrenamiento.


---

## 5. Conclusiones

- Se logr√≥ implementar exitosamente una red neuronal multicapa en C++ capaz de reconocer d√≠gitos escritos a mano del dataset MNIST, alcanzando una precisi√≥n superior al 90 % en el conjunto de prueba.
- El sistema fue dividido en dos programas principales: `train`, encargado del entrenamiento, y `predict`, dedicado exclusivamente a la inferencia a partir de pesos previamente guardados.
- El flujo completo de entrenamiento, evaluaci√≥n y predicci√≥n se ejecuta correctamente, incluyendo la visualizaci√≥n ASCII de los resultados, lo que permite una validaci√≥n visual inmediata.
- Las predicciones sobre im√°genes externas (dibujos propios) evidenciaron limitaciones en la capacidad de generalizaci√≥n del modelo, especialmente en d√≠gitos con trazos irregulares como el 9 y el 7. Esto resalta la importancia de incluir datos m√°s variados o aplicar t√©cnicas de aumento de datos en futuros trabajos.
- En general, el proyecto demuestra el potencial de construir modelos de aprendizaje profundo desde cero sin frameworks externos, reforzando el entendimiento pr√°ctico de redes neuronales, procesamiento de datos e ingenier√≠a de software en C++.

---



## 6. Trabajo en equipo



| Tarea                            | Miembro                                   | Rol                                           |
|----------------------------------|--------------------------------------------|-----------------------------------------------|
| Investigaci√≥n teorica y Implementaci√≥n del dataset | Rodrigo Miguel Gomez Pacheco - 202410309   | An√°lisis del dataset y Documentar bases te√≥ricas      |
| Dise√±o de la arquitectura        | Julio Ruiz Villavicencio - 202410616 | Organizaci√≥n estructural del sistema    |
| Implementaci√≥n del modelo final  |  Javier Alejandro Castro Barreto - 202310081      | Programaci√≥n de la red neuronal en C++        |


---



### 7. Bibliograf√≠a

[1] G. Strang, *Linear Algebra and Its Applications*, 4th ed. Belmont, CA: Brooks/Cole, 2006.

[2] C. D. Meyer, *Matrix Analysis and Applied Linear Algebra*, SIAM, 2000.

[3] A. Galindo, M. Osorio, and J. Serrano Enciso, ‚ÄúObjetos de aprendizaje para operaciones matriciales en procesamiento digital de im√°genes,‚Äù *Revista Dilemas Contempor√°neos: Educaci√≥n, Pol√≠tica y Valores*, vol. 7, no. 2, 2020.

[4] V. Cherkassky, M. Fassett, and P. Vassilas, ‚ÄúEfficient Implementation of Matrix Algorithms in C++,‚Äù *IEEE Transactions on Education*, vol. 34, no. 2, pp. 148‚Äì155, May 1991.

[5] A. Al-Ghuribi and S. Thabit, ‚ÄúA Survey on Matrix Multiplication Algorithms,‚Äù *International Journal of Computer Applications*, vol. 58, no. 19, 2012.

---
